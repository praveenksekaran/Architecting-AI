- [https://unsloth.ai/](https://unsloth.ai/)

# When to use fine-tuning?
![image](https://github.com/user-attachments/assets/e995a75a-4975-4b44-8475-0a5527057565)

# Fine-tuning for Enterprise 

[a16z](https://a16z.com/ai-enterprise-2025/)

Data/Survey from 2024 
![image](https://github.com/user-attachments/assets/1c75dee7-f39c-4532-9780-a47543cca075)

![image](https://github.com/user-attachments/assets/d2950452-c0e0-437d-b3bf-6170e10a4408)

# LLM Training Lifecycle 
![image](https://github.com/user-attachments/assets/d9c3de7b-eb95-4713-b0a8-f69290b08e04)

![image](https://github.com/user-attachments/assets/0e8c91d0-02bd-4523-af69-fe275d11a36b)

![image](https://github.com/user-attachments/assets/d4ccee30-d48b-41d2-8964-0b16ea073a40)

![image](https://github.com/user-attachments/assets/7dc92253-3f21-4ad8-8094-45dc2defecbb)

![image](https://github.com/user-attachments/assets/f0ddd6b9-0794-421f-bc5f-c5c30be781a5)

#### Supervised Fine-tuning 
![image](https://github.com/user-attachments/assets/580ebf25-9b32-40c9-a467-47b6d211079f)

# Fine-tuning process
![image](https://github.com/user-attachments/assets/23bd3e52-cd53-46a0-8624-a7a475d23d0f)

####FLHF - Reinforcement Learning from Human Feedback 
![image](https://github.com/user-attachments/assets/c5220994-e638-43c5-a17c-91c1ba23fa95)

#### DPO - Direct Preference Optimization 
![image](https://github.com/user-attachments/assets/73c2f117-3d66-43e8-95ee-390da11adc5c)

![image](https://github.com/user-attachments/assets/64eb6425-deba-4394-8938-e16c00cb05aa)























